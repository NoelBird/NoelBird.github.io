# shallow NN or/and gate 학습 in c++



yolo github을 따라서 학습 루틴을 짜고 있었습니다.

언어는 C -> C++ 컨버팅을 하고 있었습니다.

그러던 중 C++의 클래스 기능을 사용하고 싶었기 때문에, 그대로 코드를 따라할 수 없었습니다.



처음부터 객체 지향으로 짜려다 보니까 weight를 node에 붙여야 할 지, 엣지에 붙여야 할지에 대한 고민 때문에,

바로 객체를 만들기가 힘들었습니다. 고민을 하다보니, 코딩하는 속도가 느려져서

처음에 짤 때는 줄글처럼 짜고, 그 다음에 객체지향으로 변환하는 것이 어떨까 생각이 들었습니다.



저는 절차지향적 사고가 조금 더 쉬운 것 같습니다.

```c++
#include<iostream>
#include<vector>
#include<cmath>
#include<cstdlib>


float sigmoid(float x)
{
	// TODO: 자연상수 e값 math에서 찾아서 사용하기. 함수 x
	std::cout << "[in sigmoid] x: " << x << ", " << 1.f / (1.f + pow(exp(1), x)) << std::endl;
	return 1.f / (1.f + pow(exp(1), x));
}

float sigmoid_diff(float x)
{
	return -1.f / pow(1.f + pow(exp(1), x), 2) * pow(exp(1), x);
}

float predict(float x)
{
	if (x - 0.5 >= 0)
	{
		return 1;
	}
	else
	{
		return 0;
	}
}

float mse(float y, float yHat)
{
	return 1.f/2.f*pow(yHat-y, 2);
}

float mseDiff(float y, float yHat)
{
	return yHat - y;
}

float dotDiff(float input, float weight)
{
	return input;
}

int main()
{
	//TODO: 간단한 ann forward, backward 학습기 만들기
	std::vector<float> inputs;
	float y;
	inputs.resize(4);

	std::vector<float> updatesW;
	updatesW.resize(4);

	float updateB=0;
	
	//// dataset
	//inputs[0] = 0;
	//inputs[1] = 1;
	//float y = 1;

	// model
	std::vector<float> weights;
	float lr = 1.f;
	weights.resize(2);
	float bias = 0.f;
	float sum = 0.f;
	float yHat = 0.f;

	// initialization
	std::srand(0);
	for (auto& weight : weights)
	{
		weight = (double)std::rand() / RAND_MAX;
	}

	bias = (double)std::rand() / RAND_MAX;


	for (int j = 0; j < 20; ++j)
	{
		std::cout << "==================================" << std::endl;
		std::cout << j << std::endl;
		std::cout << "==================================" << std::endl;
		// forward

		// dataset
		inputs[0] = 0;
		inputs[1] = 0;
		y = 0;
		sum = 0;
		for (int i = 0; i < 2; ++i)
		{
			sum += inputs[i] * weights[i];
		}
		sum += bias;

		for (int i = 0; i < 2; ++i)
		{
			std::cout << i << "번째 input, weights: " << inputs[i] << ", " << weights[i] << std::endl;
		}
		std::cout << "bias: " << bias << std::endl;
		std::cout << "sum: " << sum << std::endl;

		yHat = sigmoid(sum);
		std::cout << "yHat: " << yHat << std::endl;
		std::cout << "predict: " << predict(yHat) << std::endl;
		std::cout << "mse: " << mse(y, yHat) << std::endl;

		// backward
		for (int i = 0; i < 2; ++i)
		{
			float dotDiffVal = dotDiff(inputs[i], weights[i]);
			updatesW[i] += lr * mseDiff(y, yHat) * sigmoid_diff(sum) * dotDiffVal;
		}
		updateB += lr * mseDiff(y, yHat) * sigmoid_diff(sum);

		// dataset
		inputs[0] = 0;
		inputs[1] = 1;
		y = 0;
		sum = 0;
		for (int i = 0; i < 2; ++i)
		{
			sum += inputs[i] * weights[i];
		}
		sum += bias;

		for (int i = 0; i < 2; ++i)
		{
			std::cout << i << "번째 input, weights: " << inputs[i] << ", " << weights[i] << std::endl;
		}
		std::cout << "bias: " << bias << std::endl;
		std::cout << "sum: " << sum << std::endl;

		yHat = sigmoid(sum);
		std::cout << "yHat: " << yHat << std::endl;
		std::cout << "predict: " << predict(yHat) << std::endl;
		std::cout << "mse: " << mse(y, yHat) << std::endl;

		// backward
		for (int i = 0; i < 2; ++i)
		{
			float dotDiffVal = dotDiff(inputs[i], weights[i]);
			updatesW[i] += lr * mseDiff(y, yHat) * sigmoid_diff(sum) * dotDiffVal;
		}
		updateB += lr * mseDiff(y, yHat) * sigmoid_diff(sum);

		// dataset
		inputs[0] = 1;
		inputs[1] = 0;
		y = 0;
		sum = 0;
		for (int i = 0; i < 2; ++i)
		{
			sum += inputs[i] * weights[i];
		}
		sum += bias;

		for (int i = 0; i < 2; ++i)
		{
			std::cout << i << "번째 input, weights: " << inputs[i] << ", " << weights[i] << std::endl;
		}
		std::cout << "bias: " << bias << std::endl;
		std::cout << "sum: " << sum << std::endl;

		yHat = sigmoid(sum);
		std::cout << "yHat: " << yHat << std::endl;
		std::cout << "predict: " << predict(yHat) << std::endl;
		std::cout << "mse: " << mse(y, yHat) << std::endl;

		// backward
		for (int i = 0; i < 2; ++i)
		{
			float dotDiffVal = dotDiff(inputs[i], weights[i]);
			updatesW[i] += lr * mseDiff(y, yHat) * sigmoid_diff(sum) * dotDiffVal;
		}
		updateB += lr * mseDiff(y, yHat) * sigmoid_diff(sum);

		// dataset
		inputs[0] = 1;
		inputs[1] = 1;
		y = 1;
		sum = 0;
		for (int i = 0; i < 2; ++i)
		{
			sum += inputs[i] * weights[i];
		}
		sum += bias;

		for (int i = 0; i < 2; ++i)
		{
			std::cout << i << "번째 input, weights: " << inputs[i] << ", " << weights[i] << std::endl;
		}
		std::cout << "bias: " << bias << std::endl;
		std::cout << "sum: " << sum << std::endl;

		yHat = sigmoid(sum);
		std::cout << "yHat: " << yHat << std::endl;
		std::cout << "predict: " << predict(yHat) << std::endl;
		std::cout << "mse: " << mse(y, yHat) << std::endl;

		// backward
		for (int i = 0; i < 2; ++i)
		{
			float dotDiffVal = dotDiff(inputs[i], weights[i]);
			updatesW[i] += lr * mseDiff(y, yHat) * sigmoid_diff(sum) * dotDiffVal;
		}
		updateB += lr * mseDiff(y, yHat) * sigmoid_diff(sum);

		for (int i = 0; i < 2; ++i)
		{
			weights[i] -= updatesW[i];
			std::cout << "weights[" << i << "]: " << weights[i] << std::endl;
		}
		bias -= updateB;
		std::cout << "bias: " << bias << std::endl;
	}
	

	return 0;
}
```



결과(20번 iteration)

AND gate의 경우(OR로 데이터를 바꿔서 넣어도 잘 동작했습니다)

```
0번째 input, weights: 0, -6.53961 // false, false => false가 나와야 함
1번째 input, weights: 0, -6.87322
bias: 10.1031
sum: 10.1031
[in sigmoid] x: 10.1031, 4.09518e-05
yHat: 4.09518e-05
predict: 0                       // false 맞음!
mse: 8.38523e-10
0번째 input, weights: 0, -6.53961 // false, true => false가 나와야 함
1번째 input, weights: 1, -6.87322
bias: 10.1031
sum: 3.22986
[in sigmoid] x: 3.22986, 0.0380576
yHat: 0.0380576
predict: 0                       // false 맞음!
mse: 0.000724189
0번째 input, weights: 1, -6.53961 // true, false => false가 나와야 함
1번째 input, weights: 0, -6.87322
bias: 10.1031
sum: 3.56346
[in sigmoid] x: 3.56346, 0.0275595
yHat: 0.0275595
predict: 0                       // false 맞음!
mse: 0.000379764
0번째 input, weights: 1, -6.53961 // true, true => true가 나와야 함
1번째 input, weights: 1, -6.87322
bias: 10.1031
sum: -3.30976
[in sigmoid] x: -3.30976, 0.964762
yHat: 0.964762
predict: 1                        // true 맞음!
mse: 0.000620855
weights[0]: -7.00273
weights[1]: -7.33888
bias: 10.7283
```



다음 step으로는 xor 구현을 해야겠습니다.